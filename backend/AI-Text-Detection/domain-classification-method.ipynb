{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Installation and Imports**","metadata":{}},{"cell_type":"code","source":"!pip install transformers --quiet\n!pip install huggingface_hub --quiet\n!pip install -U accelerate --quiet\n\n!pip install -U huggingface-hub --quiet\n!pip install datasets==2.13 --quiet\n# !pip install nlpaug","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, RobertaForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import DataCollatorWithPadding\nfrom IPython.display import FileLink, FileLinks\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nimport sklearn\nfrom sklearn.metrics import accuracy_score\nimport os\nimport numpy as np\nimport torch\nimport math\n# from torch.utils.data import DataLoader\n# import nlpaug.augmenter.word as naw","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Notebook Login**","metadata":{}},{"cell_type":"code","source":"!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token(<your_token>)\"\n\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Dataset Loading**","metadata":{}},{"cell_type":"code","source":"train_dataset = load_dataset(\"Yunij/tokenized_datasets\", split=\"train\")\ntest_dataset = load_dataset(\"Yunij/tokenized_datasets\", split=\"test\")\n\ndf_train = Dataset.to_pandas(train_dataset)\ndf_test = Dataset.to_pandas(test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.rename(columns={'label': 'human/ai'}, inplace=True)\ndf_test.rename(columns={'label': 'human/ai'}, inplace=True)\n\ndf_train = df_train.drop(['input_ids', 'attention_mask'], axis=1)\ndf_test = df_test.drop(['input_ids', 'attention_mask'], axis=1)\n\nall_sources = df_train['source'].unique().tolist()\nnum_labels = len(all_sources)\nid2label = {key: value for key, value in enumerate(all_sources)}\nlabel2id = {value: key for key, value in id2label.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['source'] = df_train['source'].map(label2id)\ndf_test['source'] = df_test['source'].map(label2id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Text Cleaning**","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nimport string\nimport subprocess\n\nnltk.download('stopwords')\n# nltk.download('wordnet')\nnltk.download('omw-1.4')\n\n# For downloading wordnet in kaggle because normal method doesn't work for wordnet in kaggle\n# Download and unzip wordnet\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n\n# Now you can import the NLTK resources as usual\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punctuations = string.punctuation\nlemmatizer = WordNetLemmatizer()\n\nstopword = stopwords.words('english')\nnew_stop = [re.sub('[^a-z]', '', word) for word in stopword] #doesn't --> doesnt, can't --> cant\nstopword.extend(new_stop)\nstopword = list(set(stopword)) #removing duplicates","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_urls(text):\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return re.sub(pattern, '', text)\n\ndef remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return re.sub(html_pattern, '', text)\n\ndef text_cleaning(text):\n    text = text.replace(\"\\n\", ' ') #removing next line\n    text = remove_urls(text) #removing urls\n    text = remove_html(text) #removing html tags\n    text = re.sub(r\"-\", \" \", text) #nearest-neighbor --> nearest neighbor, finite-size --> finite size\n    text = re.sub(r\"\\$[^$]*\\$\", \"\", text) #removing formulas written as $F = ma$\n    text = re.sub('[^a-zA-Z ]', '', text.lower()) #removing all except alphabets and spaces and changing each letter into lower case\n    text = re.sub('( . )', ' ', text) #removing a single character word\n    # text = text.translate(str.maketrans('', '', punctuations)) #removing punctuations\n    text = \" \".join([lemmatizer.lemmatize(word) for word in str(text).split() if word not in stopword]) #removing stopwords and lemmatizing\n    text = text.strip() #removing trailing spaces\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['cleaned_text'] = df_train['text'].apply(lambda x: text_cleaning(x))\ndf_test['cleaned_text'] = df_test['text'].apply(lambda x: text_cleaning(x))\n\ntrain_dataset = Dataset.from_pandas(df_train)\ntest_dataset = Dataset.from_pandas(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Fine Tuning RoBERTa**","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base',\n                                                         num_labels=num_labels, \n                                                         id2label=id2label, \n                                                         label2id=label2id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_text(examples):\n    return tokenizer(examples[\"cleaned_text\"], truncation=True, max_length=512)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.map(tokenize_text, batched=True)\ntest_dataset = test_dataset.map(tokenize_text, batched=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.push_to_hub(\"hc3-wiki-cleaned-text-for-domain-classification-roberta-tokenized-max-len-512\", split=\"train\")\ntest_dataset.push_to_hub(\"hc3-wiki-cleaned-text-for-domain-classification-roberta-tokenized-max-len-512\", split=\"test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Checkpoint for training** ","metadata":{}},{"cell_type":"code","source":"train_dataset = load_dataset(\"rajendrabaskota/hc3-wiki-cleaned-text-for-domain-classification-roberta-tokenized-max-len-512\", split=\"train\")\ntest_dataset = load_dataset(\"rajendrabaskota/hc3-wiki-cleaned-text-for-domain-classification-roberta-tokenized-max-len-512\", split=\"test\")\n\ndf_train = Dataset.to_pandas(train_dataset)\ndf_test = Dataset.to_pandas(test_dataset)\n\nall_sources = df_train['source'].unique().tolist()\nnum_labels = len(all_sources)\nid2label = {key: value for key, value in enumerate(all_sources)}\nlabel2id = {value: key for key, value in id2label.items()}\n\ntrain_dataset = train_dataset.rename_column(\"source\", \"label\")\ntest_dataset = test_dataset.rename_column(\"source\", \"label\")\n\ntrain_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ntest_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base', \n                                                         num_labels=num_labels, \n                                                         id2label=id2label, \n                                                         label2id=label2id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights = (1 - (df_train[\"source\"].value_counts().sort_index() / (len(df_train)+len(df_test)))).values\nclass_weights = torch.from_numpy(class_weights).float().to(\"cuda\")\n\nclass WeightedLossTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        # forward pass\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\nepochs = 1\noutput_dir = \"hc3-wiki-domain-classification-roberta\"\nlogging_steps = len(train_dataset) // batch_size\ntraining_args = TrainingArguments(output_dir,\n                                  num_train_epochs=epochs,\n                                  per_device_train_batch_size=batch_size,\n                                  per_device_eval_batch_size=batch_size,\n                                  evaluation_strategy=\"steps\",\n                                  eval_steps=400,\n                                  logging_strategy=\"steps\",\n                                  logging_steps=400,\n                                  learning_rate=5e-5,\n                                  weight_decay=0.01,\n#                                   save_strategy=\"no\"\n                                  save_steps=400,\n#                                   load_best_model_at_end=True,\n                                  save_total_limit=2,\n#                                   push_to_hub=False\n                                 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy_scores = []\ntest_f1_scores = []\ntrain_accuracy_scores = []\ntrain_f1_scores = []\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    print(f\"labels: {labels.shape}\")\n    print(f\"preds: {preds.shape}\")\n    accuracy = accuracy_score(labels, preds)\n    f1 = sklearn.metrics.f1_score(labels, preds, average='micro')\n    test_accuracy_scores.append(accuracy)\n    test_f1_scores.append(f1)\n    \n    return {'accuracy': accuracy, 'f1_score': f1}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = WeightedLossTrainer(model=model, \n                              args=training_args,\n                              compute_metrics=compute_metrics,\n                              train_dataset=train_dataset,\n                              eval_dataset=test_dataset,\n                              tokenizer=tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLinks(f\"hc3-wiki-domain-classification-roberta/checkpoint-10200\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub(\"hc3-wiki-domain-classification-roberta-1-epoch\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Result and Inference**","metadata":{}},{"cell_type":"code","source":"model_ckpt = \"rajendrabaskota/hc3-wiki-domain-classification-roberta\"\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_ckpt)\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\ntrainer = Trainer(model=model, \n                  tokenizer=tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = load_dataset(\"rajendrabaskota/hc3-wiki-cleaned-text-for-domain-classification-roberta-tokenized-max-len-512\", split=\"train\")\ntest_dataset = load_dataset(\"rajendrabaskota/hc3-wiki-cleaned-text-for-domain-classification-roberta-tokenized-max-len-512\", split=\"test\")\n\ndf_train = Dataset.to_pandas(train_dataset)\ndf_test = Dataset.to_pandas(test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_perplexities = df_train[['source', 'perplexity']].groupby('source').mean().to_dict()\nmean_perplexities = mean_perplexities['perplexity']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_perplexities","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum = 0\nfor key, value in mean_perplexities.items():\n    sum += value\n    \nprint(sum/6.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id2label = model.config.id2label\nmean = {}\nfor key, value in mean_perplexities.items():\n    label = id2label[key]\n    mean[label] = value\n    \nprint(mean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_metrics(df, dataset):\n    predictions, labels, _ = trainer.predict(dataset)\n    prediction_source = np.argmax(predictions, axis=-1)\n    \n    df['predicted_source'] = prediction_source\n    df['predicted_label'] = df.apply(lambda row: 1 if row['perplexity'] <= mean_perplexities[row['predicted_source']] else 0, axis=1)\n    \n    accuracy = accuracy_score(df['human/ai'], df['predicted_label'])\n    f1_score = sklearn.metrics.f1_score(df['human/ai'], df['predicted_label'], average='binary')\n    \n    return accuracy, f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_accuracy, train_f1 = calculate_metrics(df_train, train_dataset)\nprint(f\"Train Accuracy: {train_accuracy}, Train F1: {train_f1}\")\ntest_accuracy, test_f1 = calculate_metrics(df_test, test_dataset)\nprint(f\"Test Accuracy: {test_accuracy}, Test F1: {test_f1}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_metrics_source_wise(df):\n    accuracies = []\n    f1_scores = []\n    for i in range(6):\n        df_temp = df[df['source'].isin([i])]\n        accuracy = accuracy_score(df_temp['human/ai'], df_temp['predicted_label'])\n        f1_score = sklearn.metrics.f1_score(df_temp['human/ai'], df_temp['predicted_label'], average='binary')\n        \n        accuracies.append(accuracy)\n        f1_scores.append(f1_score)\n        \n    return accuracies, f1_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_accuracies, train_f1_scores = compute_metrics_source_wise(df_train)\ntest_accuracies, test_f1_scores = compute_metrics_source_wise(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_accuracies, train_f1_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracies, test_f1_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_train_acc = {}\nfinal_train_f1 = {}\nfinal_test_acc = {}\nfinal_test_f1 = {}\n\nfor i, source in enumerate(all_sources):\n    final_train_acc[source] = train_accuracies[i]\n    final_test_acc[source] = test_accuracies[i]\n    final_train_f1[source] = train_f1_scores[i]\n    final_test_f1[source] = test_f1_scores[i]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Train Accuracies: {final_train_acc}\")\nprint(f\"Train F1: {final_train_f1}\")\nprint(f\"Test Accuracies: {final_test_acc}\")\nprint(f\"Test F1: {final_test_f1}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}