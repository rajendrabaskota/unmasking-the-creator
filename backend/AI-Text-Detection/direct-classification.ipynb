{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "49344409-0f63-4086-9675-de1ac4690d1d",
    "_uuid": "37fbcde3-3d7f-4f87-b8c1-01687c0b0822"
   },
   "source": [
    "# **Unmasking the creator: Direct Classification Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9aff6cde-4a98-4ee7-a4f3-db409d473a1c",
    "_uuid": "5284ed15-821d-4b02-81ce-d18444cd3fc3"
   },
   "source": [
    "### **Loading datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "50ee9384-cfca-4806-905e-d3a1aad8db07",
    "_uuid": "939e4305-6454-4d05-a969-e1d1c87ff3ae",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "585066ff-12c4-4552-bdc6-166f522e8749",
    "_uuid": "08dadb33-640d-4fd7-b6f4-32c327edc649",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "wiki_intro_dataset = load_dataset(\"aadityaubhat/GPT-wiki-intro\", split=\"train\")\n",
    "wiki_intro_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "27e30db7-c3ab-498d-bc93-804eab5d9a37",
    "_uuid": "a3a2d6ea-49ed-468a-8de1-6feba8ca3256",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hc3_df = pd.read_csv(\"/kaggle/input/hc3-dataset/hc3_cleaned_2.csv\")\n",
    "hc3_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripping(text):\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "hc3_df['generated_texts'] = hc3_df['generated_texts'].apply(lambda x: stripping(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3b9df462-a525-4cdc-97d9-988d15def21d",
    "_uuid": "36bb1f91-122f-476f-873d-1a1840ccf888"
   },
   "source": [
    "### **Removing unnecessary columns and creating a Dataset object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e5f2b884-dea7-49ef-b676-2b8865d35ab7",
    "_uuid": "032847c6-2880-48b0-9539-fb9aebf4c21e",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cols_to_remove = wiki_intro_dataset.column_names\n",
    "cols_to_remove.remove(\"wiki_intro\")\n",
    "cols_to_remove.remove(\"generated_intro\")\n",
    "cols_to_remove.remove(\"prompt\")\n",
    "wiki_intro_dataset = wiki_intro_dataset.remove_columns(cols_to_remove)\n",
    "wiki_intro_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "30424980-10d1-4a00-bed4-62cf17deca02",
    "_uuid": "37553d06-b1ac-409a-bb69-c387c55055d3",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "wiki_intro_dataset = wiki_intro_dataset.rename_column(\"wiki_intro\", \"human_texts\")\n",
    "wiki_intro_dataset = wiki_intro_dataset.rename_column(\"generated_intro\", \"generated_texts\")\n",
    "wiki_intro_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_intro_df = Dataset.to_pandas(wiki_intro_dataset)\n",
    "wiki_intro_df['source'] = \"wiki-intro\"\n",
    "wiki_intro_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8c41f21e-e4b9-4290-84e4-8f746acc560b",
    "_uuid": "14a696a2-188f-4dc4-9da9-e726ab0e9148",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hc3_df.rename(columns = {'human_answer':'human_texts'}, inplace = True)\n",
    "hc3_df.head()\n",
    "# hc3_dataset = Dataset.from_pandas(hc3_df)\n",
    "# # cols_to_remove = [\"prompt\"]\n",
    "# # hc3_dataset = hc3_dataset.remove_columns(cols_to_remove)\n",
    "# hc3_dataset = hc3_dataset.rename_column(\"human_answer\", \"human_texts\")\n",
    "# # hc3_dataset = hc3_dataset.rename_column(\"chatgpt_answers\", \"generated_texts\")\n",
    "# hc3_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.concat([hc3_df, wiki_intro_df], axis=0)\n",
    "dataset_df = dataset_df.sample(frac=1).reset_index(drop=True)\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9ba4a3f8-2fb4-447d-b6eb-350673cdeb18",
    "_uuid": "e664e0e3-d802-4b60-a1cc-745ab4e40445",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_human = dataset_df[[\"prompt\", \"human_texts\", \"source\"]]\n",
    "dataset_ai = dataset_df[[\"prompt\", \"generated_texts\", \"source\"]]\n",
    "\n",
    "dataset_human = dataset_human.assign(label=0)\n",
    "dataset_ai = dataset_ai.assign(label=1)\n",
    "\n",
    "dataset_human.rename(columns={'human_texts': 'text'}, inplace=True)\n",
    "dataset_ai.rename(columns={'generated_texts': 'text'}, inplace=True)\n",
    "\n",
    "df = pd.concat([dataset_human, dataset_ai], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True) # shuffling the rows\n",
    "# df.to_csv(\"wiki_and_hc3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.05, stratify=df['source'])\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.label.value_counts(), df_test.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_test = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub --quiet\n",
    "# !pip install -U accelerate --quiet\n",
    "\n",
    "# !pip install -U huggingface-hub --quiet\n",
    "# !pip install datasets==2.13 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token(<your_token)\"\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.push_to_hub(\"hc3-wiki-intro-dataset\", split=\"train\")\n",
    "dataset_test.push_to_hub(\"hc3-wiki-intro-dataset\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "25de4ea4-668f-4ce3-b19e-0a15ea0772b4",
    "_uuid": "a62a9792-98cb-473f-a8c7-8171cf9c8569"
   },
   "source": [
    "# **Training Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d05ba281-8385-4d19-bd8a-838701fd603a",
    "_uuid": "916c89db-96e2-45e6-8847-772e64c3f7c9"
   },
   "source": [
    "## **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df['text']\n",
    "X_test = test_df['text']\n",
    "\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = [0.7, 0.5, 0.3]\n",
    "# max_iter = [100, 200, 300]\n",
    "\n",
    "# for c in C:\n",
    "#     for iters in max_iter:\n",
    "#         log_reg = LogisticRegression(C=c, max_iter=iters)\n",
    "#         log_reg.fit(X_train_tfidf, y_train)\n",
    "#         y_pred_train = log_reg.predict(X_train_tfidf)\n",
    "#         y_pred_test = log_reg.predict(X_test_tfidf)\n",
    "#         print(f\"for C={c} and max_iter={iters}\")\n",
    "#         print(\"ACCURACY\")\n",
    "#         print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train)}\")\n",
    "#         print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test)}\")\n",
    "#         print(\"\\nF1 SCORE\")\n",
    "#         print(f\"Train F1: {f1_score(y_train, y_pred_train)}\")\n",
    "#         print(f\"Test F1: {f1_score(y_test, y_pred_test)}\")\n",
    "#         print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SGD Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss='log_loss', alpha=0.0001, max_iter=500, tol=None)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "y_pred_train = clf.predict(X_train_tfidf)\n",
    "y_pred_test = clf.predict(X_test_tfidf)\n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train)}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fine Tuning RoBERTa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers --quiet\n",
    "!pip install huggingface_hub --quiet\n",
    "!pip install -U accelerate --quiet\n",
    "\n",
    "!pip install -U huggingface-hub --quiet\n",
    "!pip install datasets==2.13 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from IPython.display import FileLink, FileLinks\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"rajendrabaskota/hc3-wiki-intro-tokenized-max-len-512\", split=\"train\")\n",
    "test_dataset = load_dataset(\"rajendrabaskota/hc3-wiki-intro-tokenized-max-len-512\", split=\"test\")\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of parameters: {sum(p.numel() for p in model.parameters())/1e6} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 1\n",
    "output_dir = \"ai-human-classification-hc3-wiki-recleaned-dataset-max-length-512\"\n",
    "logging_steps = len(train_dataset) // batch_size\n",
    "training_args = TrainingArguments(output_dir,\n",
    "                                  num_train_epochs=epochs,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  evaluation_strategy=\"steps\",\n",
    "                                  eval_steps=100,\n",
    "                                  logging_strategy=\"steps\",\n",
    "                                  logging_steps=100,\n",
    "                                  learning_rate=5e-5,\n",
    "                                  weight_decay=0.01,\n",
    "                                  save_strategy=\"no\"\n",
    "#                                   save_steps=300,\n",
    "#                                   load_best_model_at_end=True,\n",
    "#                                   save_total_limit=2,\n",
    "#                                   push_to_hub=False\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_scores = []\n",
    "test_f1_scores = []\n",
    "\n",
    "train_accuracy_scores = []\n",
    "train_f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    print(f\"labels: {labels.shape}\")\n",
    "    print(f\"preds: {preds.shape}\")\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = sklearn.metrics.f1_score(labels, preds, average='binary')\n",
    "    test_accuracy_scores.append(accuracy)\n",
    "    test_f1_scores.append(f1)\n",
    "    \n",
    "    return {'accuracy': accuracy, 'f1_score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, \n",
    "                  args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset=test_dataset,\n",
    "                 tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLinks(f\"ai-human-classification-hc3-wiki-recleaned-dataset/checkpoint-200\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
